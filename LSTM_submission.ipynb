{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yunkun\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import LSTM, GRU, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_1(ftrain, ftest, max_features=20000, maxlen=100):\n",
    "    train = pd.read_csv(ftrain)\n",
    "    test  = pd.read_csv(ftest)\n",
    "\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\" \").values\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\" \").values\n",
    "    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "    train_sentence_filtered = train[   (train.toxic == 1)   | (train.severe_toxic == 1) \\\n",
    "                                 | (train.obscene == 1) | (train.threat == 1)   \\\n",
    "                                 | (train.insult ==  1) | (train.identity_hate == 1) ]\n",
    "    list_filtered_train = train_sentence_filtered[\"comment_text\"].fillna(\" \").values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(list_filtered_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "    X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    y = train[list_classes].values\n",
    "    return X_train, X_test, y\n",
    "\n",
    "def preprocess_2(ftrain, ftest, max_features=20000, maxlen=100):\n",
    "    train_orig = pd.read_csv(ftrain)\n",
    "    test_orig  = pd.read_csv(ftest)\n",
    "\n",
    "    train_effective_samples = train_orig[   (train_orig.toxic == 1)   | (train_orig.severe_toxic == 1) \\\n",
    "                                        | (train_orig.obscene == 1) | (train_orig.threat == 1)   \\\n",
    "                                      | (train_orig.insult ==  1) | (train_orig.identity_hate == 1) ]\n",
    "    # repeat the threat data at the beginning\n",
    "    threat_samples = train_orig[train_orig.threat == 1]\n",
    "    word_fit       = threat_samples.append(train_effective_samples)\n",
    "    train_combined = threat_samples.append(train_orig)\n",
    "    \n",
    "    list_sentences_fit = word_fit[\"comment_text\"].fillna(\" \").values\n",
    "    list_sentences_train = train_combined[\"comment_text\"].fillna(\" \").values\n",
    "    list_sentences_test = test_orig[\"comment_text\"].fillna(\" \").values\n",
    "    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(list_sentences_fit))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "    X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    y = train_combined[list_classes].values\n",
    "    return X_train, X_test, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./input/train.csv\"\n",
    "test_file  = \"./input/test.csv\"\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "X_train, X_test, y = preprocess_2(train_file, test_file, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Keras model graph\n",
    "def get_model():\n",
    "    embed_size = 600\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(GRU(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144044 samples, validate on 16005 samples\n",
      "Epoch 1/3\n",
      "144032/144044 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9790\n",
      "Epoch 00001: val_loss improved from inf to 0.04678, saving model to weights_GRU_021718.best.hdf5\n",
      "144044/144044 [==============================] - 1295s 9ms/step - loss: 0.0610 - acc: 0.9790 - val_loss: 0.0468 - val_acc: 0.9822\n",
      "Epoch 2/3\n",
      "144032/144044 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9843\n",
      "Epoch 00002: val_loss improved from 0.04678 to 0.04397, saving model to weights_GRU_021718.best.hdf5\n",
      "144044/144044 [==============================] - 1290s 9ms/step - loss: 0.0408 - acc: 0.9843 - val_loss: 0.0440 - val_acc: 0.9836\n",
      "Epoch 3/3\n",
      "144032/144044 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9871\n",
      "Epoch 00003: val_loss did not improve\n",
      "144044/144044 [==============================] - 1291s 9ms/step - loss: 0.0334 - acc: 0.9871 - val_loss: 0.0477 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "file_path=\"weights_GRU_021718.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks_list = [checkpoint, early] \n",
    "\n",
    "#model.load_weights(file_path)\n",
    "history = model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history.history['val_loss'],'o')\n",
    "\n",
    "#with open('./trainHistory_021418', 'wb') as file:\n",
    "#    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "file_path=\"weights_submission_021618.best.hdf5\"\n",
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission = pd.read_csv(\"./input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"sub_021618.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "file_path=\"weights_submission_021618.best.hdf5\"\n",
    "model.load_weights(file_path)\n",
    "y_pred = model.predict(X_train[-10:-1])\n",
    "print(type(y_pred), type(y[-10:-1]))\n",
    "score = K.eval(losses.binary_crossentropy(y[-10:-1], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
